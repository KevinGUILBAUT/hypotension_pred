{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e46347d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pret.PORTPRETLAMIH11\\Desktop\\test\\env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff18b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'signal_dataset_chu'\n",
    "\n",
    "data = pd.read_parquet(f'./data/datasets/{dataset_name}/cases/', engine='pyarrow')\n",
    "static = pd.read_parquet(f'./data/datasets/{dataset_name}/meta.parquet', engine='pyarrow')\n",
    "data = data.merge(static, on='caseid')\n",
    "\n",
    "data[\"last_map_value\"] = data[\"mbp_19\"]\n",
    "data = data[data['ioh_in_leading_time']==0]\n",
    "data = data[data['ioh_at_time_t']==0]\n",
    "data = data[data['intervention']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98ea3eb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mbp': [],\n",
       " 'sbp': [],\n",
       " 'dbp': [],\n",
       " 'hr': [],\n",
       " 'rr': [],\n",
       " 'spo2': [7, 8, 9, 10, 11, 12],\n",
       " 'etco2': [],\n",
       " 'mac': [],\n",
       " 'pp_ct': [],\n",
       " 'rf_ct': [],\n",
       " 'body_temp': []}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signal_types = ['mbp', 'sbp', 'dbp', 'hr', 'rr', 'spo2', 'etco2', 'mac', 'pp_ct', 'rf_ct', 'body_temp']\n",
    "\n",
    "def get_signal_time(signal):\n",
    "    try:\n",
    "        # Tries to get the last part after splitting by '_'\n",
    "        return int(signal.split('_')[-1])\n",
    "    except ValueError:\n",
    "        return 0\n",
    "    except IndexError:\n",
    "        return 0\n",
    "\n",
    "# Group columns by signal type and sort them by their time index\n",
    "grouped_columns = {\n",
    "    signal: sorted([col for col in data.columns if col.startswith(signal)], key=get_signal_time)\n",
    "    for signal in signal_types\n",
    "}\n",
    "\n",
    "other_columns = ['age','bmi','asa',\"last_map_value\",\"label\", \"label_id\", 'caseid', 'time',]\n",
    "\n",
    "# --- Selecting the desired signal columns ---\n",
    "selected_signal_columns = []\n",
    "for signal_type in signal_types:\n",
    "    # Get the sorted columns for the current signal type\n",
    "    cols_for_signal = grouped_columns.get(signal_type, [])\n",
    "    selected_signal_columns.extend(cols_for_signal[:20])\n",
    "\n",
    "all_desired_columns = selected_signal_columns + other_columns\n",
    "data = data.reindex(columns=all_desired_columns)\n",
    "\n",
    "def check_signal_time_steps(data: pd.DataFrame, signal_types: list, max_time_step: int = 19):\n",
    "    \"\"\"\n",
    "    Checks if all expected time steps (0 to max_time_step) are present\n",
    "    for each specified signal type in the DataFrame.\n",
    "    \"\"\"\n",
    "    missing_steps_by_signal = {}\n",
    "\n",
    "    for signal_type in signal_types:\n",
    "        expected_columns = {f\"{signal_type}_{i}\" for i in range(max_time_step + 1)}\n",
    "        \n",
    "        # Get all columns in the DataFrame that start with the current signal type\n",
    "        present_columns_for_signal = {col for col in data.columns if col.startswith(signal_type)}\n",
    "        \n",
    "        # Find which expected columns are not present\n",
    "        missing_columns = expected_columns - present_columns_for_signal\n",
    "        \n",
    "        if missing_columns:\n",
    "            # Extract just the time steps from the missing column names\n",
    "            missing_time_steps = sorted([int(col.split('_')[-1]) for col in missing_columns])\n",
    "            missing_steps_by_signal[signal_type] = missing_time_steps\n",
    "        else:\n",
    "            missing_steps_by_signal[signal_type] = [] \n",
    "\n",
    "    return missing_steps_by_signal\n",
    "\n",
    "missing_steps = check_signal_time_steps(data,signal_types)\n",
    "missing_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114ea3a1",
   "metadata": {},
   "source": [
    "For all the dataset, spo2 is missing, I will then average the 6th and 13th for every timesteps between."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46868cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La colonne 'spo2_7' a été ajoutée et imputée avec la moyenne de spo2_6 et spo2_13.\n",
      "La colonne 'spo2_8' a été ajoutée et imputée avec la moyenne de spo2_6 et spo2_13.\n",
      "La colonne 'spo2_9' a été ajoutée et imputée avec la moyenne de spo2_6 et spo2_13.\n",
      "La colonne 'spo2_10' a été ajoutée et imputée avec la moyenne de spo2_6 et spo2_13.\n",
      "La colonne 'spo2_11' a été ajoutée et imputée avec la moyenne de spo2_6 et spo2_13.\n",
      "La colonne 'spo2_12' a été ajoutée et imputée avec la moyenne de spo2_6 et spo2_13.\n"
     ]
    }
   ],
   "source": [
    "# Colonnes manquantes pour 'spo2'\n",
    "missing_spo2_steps = [7, 8, 9, 10, 11, 12]\n",
    "\n",
    "# --- Calcul de la valeur d'imputation ---\n",
    "if f'spo2_6' in data.columns and f'spo2_13' in data.columns:\n",
    "    imputation_value = data[[f'spo2_6', f'spo2_13']].mean(axis=1)\n",
    "\n",
    "    # Impute les colonnes manquantes\n",
    "    for step in missing_spo2_steps:\n",
    "        col_name = f'spo2_{step}'\n",
    "        if col_name not in data.columns: # Vérifie si la colonne n'existe pas déjà pour éviter de l'écraser\n",
    "            data[col_name] = imputation_value\n",
    "            print(f\"La colonne '{col_name}' a été ajoutée et imputée avec la moyenne de spo2_6 et spo2_13.\")\n",
    "        else:\n",
    "            print(f\"La colonne '{col_name}' existe déjà. Aucune imputation n'a été effectuée.\")\n",
    "else:\n",
    "    print(\"Les colonnes 'spo2_6' ou 'spo2_13' (ou les deux) sont manquantes. Impossible de calculer la moyenne pour l'imputation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3653d9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_directory = 'data/datasets/clean_chu_trends'\n",
    "output_filepath = os.path.join(output_directory, 'data.parquet')\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Save the DataFrame to a Parquet file\n",
    "data.to_parquet(output_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427b7aee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env (3.11.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
